import os
import argparse
import wandb
from model import train
from inference_ens import single_model_inference, ensemble_inference
import itertools
import pandas as pd
from datetime import datetime
import sys

def parse_args():
    """í•™ìŠµ(train)ê³¼ ì¶”ë¡ (infer)ì— ì‚¬ìš©ë˜ëŠ” argumentsë¥¼ ê´€ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Training and Inference arguments")

    parser.add_argument("--dataset_dir", type=str, default="./NIKL_AU_2023_COMPETITION_v1.0")
    parser.add_argument("--model_name", type=str, default="beomi/KcELECTRA-base-v2022")
    parser.add_argument("--save_path", type=str, default="./model")
    parser.add_argument("--save_step", type=int, default=200)
    parser.add_argument("--logging_step", type=int, default=200)
    parser.add_argument("--eval_step", type=int, default=200)
    parser.add_argument("--save_limit", type=int, default=5)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--max_len", type=int, default=256)
    parser.add_argument("--lr", type=float, default=3e-5)
    parser.add_argument("--weight_decay", type=float, default=0.01)
    parser.add_argument("--warmup_steps", type=int, default=300)
    parser.add_argument("--model_dir", type=str, default="./best_model")
    parser.add_argument("--run_name", type=str, default="bert-test")
    parser.add_argument("--eval_file", type=str, default="dev.csv", choices=['train.csv', 'dev.csv', 'test.csv'])
    parser.add_argument("--mode", type=str, default="advanced_ensemble", 
                       choices=['train', 'single_inference', 'advanced_ensemble', 'test_submission'])
    parser.add_argument("--save_jsonl", action='store_true', default=True)

    return parser.parse_args()


def print_header(title, width=80):
    """ê¹”ë”í•œ í—¤ë” ì¶œë ¥"""
    print("\n" + "=" * width)
    print(f"{title:^{width}}")
    print("=" * width)


def evaluate_single_models(args, all_models):
    """ëª¨ë“  ë‹¨ì¼ ëª¨ë¸ì„ í‰ê°€í•˜ê³  ì„±ëŠ¥ì„ ê¸°ë¡"""
    print_header("ë‹¨ì¼ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    
    model_performances = {}
    
    for i, model_name in enumerate(all_models, 1):
        model_short = model_name.split('/')[-1]
        print(f"\n[{i}/{len(all_models)}] {model_short}...", end=' ', flush=True)
        
        try:
            acc, f1, record_time = single_model_inference(args, model_name, args.eval_file, save_jsonl=args.save_jsonl)
            model_performances[i] = {
                'model_name': model_name,
                'accuracy': acc,
                'f1_score': f1,
                'record_time': record_time
            }
            
            print(f"âœ“ Acc: {acc*100:5.1f}% | F1: {f1*100:5.1f}%")
            
        except Exception as e:
            print(f"âœ— ì‹¤íŒ¨")
            model_performances[i] = {
                'model_name': model_name,
                'accuracy': 0.0,
                'f1_score': 0.0,
                'record_time': None
            }
    
    # ì„±ëŠ¥ ê²°ê³¼ ì €ì¥ ë° ìš”ì•½ ì¶œë ¥
    results_df = pd.DataFrame.from_dict(model_performances, orient='index')
    timestamp = datetime.now().strftime('%m_%d_%H_%M')
    results_file = f"./single_model_performances_{timestamp}.csv"
    results_df.to_csv(results_file, index=True)
    
    print_header("ë‹¨ì¼ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½")
    print(f"{'ìˆœìœ„':<6} {'ëª¨ë¸ëª…':<40} {'Accuracy':<12} {'F1 Score':<12}")
    print("-" * 75)
    
    sorted_performances = sorted(model_performances.items(), 
                               key=lambda x: x[1]['f1_score'], reverse=True)
    
    for rank, (i, perf) in enumerate(sorted_performances, 1):
        model_short = perf['model_name'].split('/')[-1]
        if len(model_short) > 35:
            model_short = model_short[:32] + "..."
        print(f"{rank:<6} {model_short:<40} {perf['accuracy']*100:5.1f}%        {perf['f1_score']*100:5.1f}%")
    
    print(f"\nâœ“ ê²°ê³¼ ì €ì¥: {results_file}")
    return model_performances


def find_best_model(model_performances, metric='f1_score'):
    """ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    best_model_id = max(model_performances.keys(), 
                       key=lambda x: model_performances[x][metric])
    best_model_info = model_performances[best_model_id]
    
    print(f"\nâœ¨ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: #{best_model_id} {best_model_info['model_name']}")
    print(f"   {metric.upper()}: {best_model_info[metric]:.4f} ({best_model_info[metric]*100:.1f}%)")
    
    return best_model_id, best_model_info


def generate_ensemble_combinations(best_model_id, all_models):
    """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  ì•™ìƒë¸” ì¡°í•© ìƒì„±"""
    other_model_indices = [i for i in range(1, len(all_models) + 1) if i != best_model_id]
    combinations = []
    
    # 2ê°œ ì¡°í•© (best + 1ê°œ)
    for other_id in other_model_indices:
        combinations.append([best_model_id, other_id])
    
    # 3ê°œ ì¡°í•© (best + 2ê°œ)
    for combo in itertools.combinations(other_model_indices, 2):
        combinations.append([best_model_id] + list(combo))
    
    # 4ê°œ ì¡°í•© (best + 3ê°œ)
    for combo in itertools.combinations(other_model_indices, 3):
        combinations.append([best_model_id] + list(combo))
    
    # 5ê°œ ì¡°í•© (best + 4ê°œ = ì „ì²´)
    combinations.append([best_model_id] + other_model_indices)
    
    return combinations


def find_best_ensemble_interactive(ensemble_results, all_models):
    """ìµœê³  ì„±ëŠ¥ ì•™ìƒë¸” ì¡°í•© ì°¾ê¸° (ë™ì  ì‹œ ì‚¬ìš©ì ì„ íƒ)"""
    if not ensemble_results:
        return None, None
    
    # âœ¨ ìˆ˜ì • 1: ì „ì²´ì—ì„œ ìµœê³  F1 ì°¾ê¸° (Hard/Soft í†µí•©)
    max_f1 = 0
    best_voting_type = None
    
    for res in ensemble_results.values():
        if res['hard_voting_f1'] > max_f1:
            max_f1 = res['hard_voting_f1']
            best_voting_type = 'hard'
        if res['soft_voting_f1'] > max_f1:
            max_f1 = res['soft_voting_f1']
            best_voting_type = 'soft'
    
    print(f"\nâœ¨ ì „ì²´ ìµœê³  F1 Score: {max_f1*100:.2f}% ({best_voting_type.upper()} Voting)")
    
    # âœ¨ ìˆ˜ì • 2: ìµœê³  F1ì„ ê°€ì§„ ëª¨ë“  ì¡°í•© ì°¾ê¸° (Hard/Soft êµ¬ë¶„ ì—†ì´)
    candidates = []
    
    for name, res in ensemble_results.items():
        # Hard Votingì´ ìµœê³  F1ê³¼ ê°™ì€ ê²½ìš°
        if abs(res['hard_voting_f1'] - max_f1) < 0.0001:
            candidates.append((name, res, 'Hard Voting', res['hard_voting_f1'], res['hard_voting_acc']))
        
        # Soft Votingì´ ìµœê³  F1ê³¼ ê°™ì€ ê²½ìš°
        if abs(res['soft_voting_f1'] - max_f1) < 0.0001:
            candidates.append((name, res, 'Soft Voting', res['soft_voting_f1'], res['soft_voting_acc']))
    
    # âœ¨ ìˆ˜ì • 3: ë™ì  ì²˜ë¦¬ ë‹¨ìˆœí™”
    if len(candidates) == 1:
        # ë™ì  ì—†ìŒ - ë°”ë¡œ ì„ íƒ
        name, res, voting_type, f1, acc = candidates[0]
        print(f"â†’ ìœ ì¼í•œ ìµœê³  ì¡°í•© ìë™ ì„ íƒ")
        return (name, res), voting_type
    
    else:
        # ë™ì  ìˆìŒ - ì‚¬ìš©ì ì„ íƒ
        print("\n" + "="*80)
        print(f"âš ï¸  ìµœê³  F1 ì ìˆ˜({max_f1*100:.2f}%)ë¥¼ ê°€ì§„ ì¡°í•©ì´ {len(candidates)}ê°œ ìˆìŠµë‹ˆë‹¤.")
        print("="*80)
        
        # Accuracy ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        candidates.sort(key=lambda x: x[4], reverse=True)
        
        print("\në™ì  ì¡°í•© ëª©ë¡ (Accuracy ìˆœ):")
        for idx, (name, res, voting_type, f1, acc) in enumerate(candidates, 1):
            model_names = [all_models[i-1].split('/')[-1] for i in res['combination']]
            print(f"  [{idx}] ì¡°í•© {res['combination']} - {voting_type}")
            print(f"      F1: {f1*100:.2f}% | Acc: {acc*100:.2f}%")
            print(f"      ëª¨ë¸: {', '.join(model_names)}")
        
        while True:
            try:
                choice = input(f"\nì„ íƒí•˜ì„¸ìš” (1-{len(candidates)}) [Enter = Accuracy ìµœê³ ]: ").strip()
                if choice == "":
                    selected_idx = 0
                    print(f"â†’ Accuracyê°€ ê°€ì¥ ë†’ì€ ì¡°í•© ì„ íƒ ({candidates[0][4]*100:.2f}%)")
                    break
                else:
                    selected_idx = int(choice) - 1
                    if 0 <= selected_idx < len(candidates):
                        print(f"â†’ [{choice}]ë²ˆ ì¡°í•© ì„ íƒ")
                        break
                    else:
                        print(f"âŒ 1-{len(candidates)} ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            except ValueError:
                print("âŒ ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        
        name, res, voting_type, f1, acc = candidates[selected_idx]
        return (name, res), voting_type

def find_best_ensemble_comprehensive(ensemble_results, all_models):
    """Hard ìµœê³ , Soft ìµœê³ , ì „ì²´ ìµœê³  ëª¨ë‘ ì°¾ê¸°"""
    if not ensemble_results:
        return None, None, None
    
    # 1. Hard Voting ìµœê³  ì°¾ê¸°
    max_hard_f1 = max(res['hard_voting_f1'] for res in ensemble_results.values())
    hard_candidates = [
        (name, res) for name, res in ensemble_results.items()
        if abs(res['hard_voting_f1'] - max_hard_f1) < 0.0001
    ]
    hard_candidates.sort(key=lambda x: x[1]['hard_voting_acc'], reverse=True)
    best_hard = hard_candidates[0] if hard_candidates else None
    
    # 2. Soft Voting ìµœê³  ì°¾ê¸°
    max_soft_f1 = max(res['soft_voting_f1'] for res in ensemble_results.values())
    soft_candidates = [
        (name, res) for name, res in ensemble_results.items()
        if abs(res['soft_voting_f1'] - max_soft_f1) < 0.0001
    ]
    soft_candidates.sort(key=lambda x: x[1]['soft_voting_acc'], reverse=True)
    best_soft = soft_candidates[0] if soft_candidates else None
    
    # 3. ì „ì²´ ìµœê³  (ì •ë³´ìš©)
    max_f1 = max(max_hard_f1, max_soft_f1)
    best_overall_type = 'Hard' if max_hard_f1 >= max_soft_f1 else 'Soft'
    
    return best_hard, best_soft, (max_f1, best_overall_type)

def run_advanced_ensemble(args, all_models):
    """ê³ ê¸‰ ì•™ìƒë¸” ì „ëµ ì‹¤í–‰"""
    
    record_time = datetime.now().strftime("%m_%d_%H_%M")
    
    # 1-3ë‹¨ê³„: ë™ì¼
    model_performances = evaluate_single_models(args, all_models)
    best_model_id, best_model_info = find_best_model(model_performances, 'f1_score')
    combinations = generate_ensemble_combinations(best_model_id, all_models)
    
    print_header(f"ì•™ìƒë¸” ì‹¤í—˜ ({len(combinations)}ê°€ì§€ ì¡°í•©)")
    ensemble_results = {}
    
    # ì•™ìƒë¸” ì‹¤í—˜ (dev.csvë¡œ í‰ê°€ë§Œ)
    for i, combo in enumerate(combinations, 1):
        combo_models = [all_models[idx-1] for idx in combo]
        combo_name = f"best({best_model_id})+{'+'.join(map(str, [x for x in combo if x != best_model_id]))}"
        print(f"[{i:2d}/{len(combinations)}] {len(combo)}ê°œ ëª¨ë¸...", end=' ', flush=True)
        
        try:
            hard_results, soft_results = ensemble_inference(
                args, combo_models, record_time, args.eval_file, save_jsonl=False
            )
            
            if hard_results and soft_results:
                hard_acc, hard_f1 = hard_results
                soft_acc, soft_f1 = soft_results
                
                ensemble_results[combo_name] = {
                    'combination': combo,
                    'models': combo_models,
                    'hard_voting_acc': hard_acc,
                    'hard_voting_f1': hard_f1,
                    'soft_voting_acc': soft_acc,
                    'soft_voting_f1': soft_f1,
                    'num_models': len(combo)
                }
                
                print(f"âœ“ Hard F1: {hard_f1*100:5.1f}% | Soft F1: {soft_f1*100:5.1f}%")
            else:
                print("âœ— ì‹¤íŒ¨")
        except Exception as e:
            print(f"âœ— ì˜¤ë¥˜")
    
    # ê²°ê³¼ ì €ì¥
    if ensemble_results:
        results_df = pd.DataFrame.from_dict(ensemble_results, orient='index')
        results_file = f"./ensemble_results_{record_time}.csv"
        results_df.to_csv(results_file, index=True)
        
        print_header("ìµœì¢… ì•™ìƒë¸” ê²°ê³¼")
        
        # Hard/Soft ìµœê³  ì°¾ê¸°
        best_hard, best_soft, _ = find_best_ensemble_comprehensive(ensemble_results, all_models)
        
        # Hard Voting ìµœê³ 
        if best_hard:
            hard_name, hard_res = best_hard
            print("\nğŸ¥‡ Hard Voting ìµœê³  ì¡°í•©:")
            print(f"   ì¡°í•©: {hard_res['combination']}")
            print(f"   F1: {hard_res['hard_voting_f1']*100:.2f}% | Acc: {hard_res['hard_voting_acc']*100:.2f}%")
            hard_model_names = [all_models[i-1].split('/')[-1] for i in hard_res['combination']]
            print(f"   ëª¨ë¸: {', '.join(hard_model_names)}")
        
        # Soft Voting ìµœê³ 
        if best_soft:
            soft_name, soft_res = best_soft
            print("\nğŸ¥ˆ Soft Voting ìµœê³  ì¡°í•©:")
            print(f"   ì¡°í•©: {soft_res['combination']}")
            print(f"   F1: {soft_res['soft_voting_f1']*100:.2f}% | Acc: {soft_res['soft_voting_acc']*100:.2f}%")
            soft_model_names = [all_models[i-1].split('/')[-1] for i in soft_res['combination']]
            print(f"   ëª¨ë¸: {', '.join(soft_model_names)}")
        
        # ì „ì²´ ë¹„êµ (ì •ë³´ë§Œ ì¶œë ¥)
        if best_hard and best_soft:
            hard_f1 = hard_res['hard_voting_f1']
            soft_f1 = soft_res['soft_voting_f1']
            
            print("\n" + "="*80)
            print("ğŸ“Š Hard vs Soft ë¹„êµ")
            print("="*80)
            
            if hard_f1 > soft_f1:
                winner = "Hard Voting"
                winner_f1 = hard_f1
                winner_acc = hard_res['hard_voting_acc']
                diff = hard_f1 - soft_f1
            elif soft_f1 > hard_f1:
                winner = "Soft Voting"
                winner_f1 = soft_f1
                winner_acc = soft_res['soft_voting_acc']
                diff = soft_f1 - hard_f1
            else:
                winner = "ë™ì "
                winner_f1 = hard_f1
                winner_acc = max(hard_res['hard_voting_acc'], soft_res['soft_voting_acc'])
                diff = 0
            
            print(f"   Hard: F1 {hard_f1*100:.2f}% | Acc {hard_res['hard_voting_acc']*100:.2f}%")
            print(f"   Soft: F1 {soft_f1*100:.2f}% | Acc {soft_res['soft_voting_acc']*100:.2f}%")
            print(f"\n   ğŸ† ìµœì¢… ìš°ìŠ¹: {winner}")
            print(f"      F1: {winner_f1*100:.2f}% | Acc: {winner_acc*100:.2f}%")
            if diff > 0:
                print(f"      ê²©ì°¨: +{diff*100:.2f}%p")
            
            # ë‹¨ì¼ ëª¨ë¸ ëŒ€ë¹„ í–¥ìƒë„
            best_single_f1 = max(model_performances.values(), key=lambda x: x['f1_score'])['f1_score']
            improvement = winner_f1 - best_single_f1
            
            print(f"\n   ğŸ“ˆ ë‹¨ì¼ ëª¨ë¸ ëŒ€ë¹„:")
            print(f"      ë‹¨ì¼ ëª¨ë¸ ìµœê³ : {best_single_f1*100:.2f}%")
            print(f"      ì•™ìƒë¸” ìµœê³ :    {winner_f1*100:.2f}%")
            print(f"      í–¥ìƒë„:         {improvement*100:+.2f}%p")
        
        print(f"\nâœ“ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")
        
        # âœ¨ test.csvë¡œ Hard/Soft ìµœê³  ì¡°í•© JSONL ìƒì„±
        print("\n" + "="*80)
        print("ğŸ“¦ ì œì¶œìš© JSONL íŒŒì¼ ìƒì„± ì¤‘...")
        print("="*80)
        
        test_path = os.path.join(args.dataset_dir, "test.csv")
        if os.path.exists(test_path):
            submission_time = datetime.now().strftime("%m_%d_%H_%M")
            
            # Hard ìµœê³  ì¡°í•© JSONL
            if best_hard:
                hard_name, hard_res = best_hard
                hard_models = hard_res['models']
                print(f"\n1ï¸âƒ£  Hard Voting ìµœê³  ì¡°í•© â†’ ì œì¶œ íŒŒì¼ ìƒì„±")
                print(f"   ì¡°í•©: {hard_res['combination']}")
                print(f"   ëª¨ë¸: {', '.join([m.split('/')[-1] for m in hard_models])}")
                try:
                    ensemble_inference(
                        args, hard_models, submission_time + "_hard_best", 
                        "test.csv", save_jsonl=True
                    )
                    print(f"   âœ… hard_voting_ensemble_{len(hard_models)}_{submission_time}_hard_best.jsonl")
                except Exception as e:
                    print(f"   âŒ ìƒì„± ì‹¤íŒ¨: {str(e)[:50]}")
            
            # Soft ìµœê³  ì¡°í•© JSONL
            if best_soft:
                soft_name, soft_res = best_soft
                soft_models = soft_res['models']
                print(f"\n2ï¸âƒ£  Soft Voting ìµœê³  ì¡°í•© â†’ ì œì¶œ íŒŒì¼ ìƒì„±")
                print(f"   ì¡°í•©: {soft_res['combination']}")
                print(f"   ëª¨ë¸: {', '.join([m.split('/')[-1] for m in soft_models])}")
                try:
                    ensemble_inference(
                        args, soft_models, submission_time + "_soft_best", 
                        "test.csv", save_jsonl=True
                    )
                    print(f"   âœ… soft_voting_ensemble_{len(soft_models)}_{submission_time}_soft_best.jsonl")
                except Exception as e:
                    print(f"   âŒ ìƒì„± ì‹¤íŒ¨: {str(e)[:50]}")
            
            print("\n" + "="*80)
            print("âœ… ì œì¶œìš© íŒŒì¼ ìƒì„± ì™„ë£Œ!")
            print("="*80)
            print("ğŸ“ ./prediction/ í´ë”ì—ì„œ ë‹¤ìŒ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”:")
            print(f"   1. hard_voting_ensemble_{len(hard_models)}_{submission_time}_hard_best.jsonl")
            print(f"   2. soft_voting_ensemble_{len(soft_models)}_{submission_time}_soft_best.jsonl")
            print("\nğŸ’¡ ë¦¬ë”ë³´ë“œ ì œì¶œ íŒ:")
            if winner == "Hard Voting":
                print(f"   â†’ Hardê°€ dev.csvì—ì„œ ë” ë†’ì•˜ìœ¼ë‹ˆ Hard ë¨¼ì € ì œì¶œ ì¶”ì²œ!")
            elif winner == "Soft Voting":
                print(f"   â†’ Softê°€ dev.csvì—ì„œ ë” ë†’ì•˜ìœ¼ë‹ˆ Soft ë¨¼ì € ì œì¶œ ì¶”ì²œ!")
            else:
                print(f"   â†’ ë‘˜ ë‹¤ ë™ì ì´ë‹ˆ ë‘˜ ë‹¤ ì œì¶œí•´ë³´ì„¸ìš”!")
        else:
            print(f"\nâš ï¸  test.csvë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_path}")
    else:
        print("\nâœ— ì•™ìƒë¸” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")


def run_test_submission(args, all_models):
    """test.csvì— ëŒ€í•´ ìµœì¢… ì œì¶œìš© ì¶”ë¡  ì‹¤í–‰"""
    print_header("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì œì¶œìš© ì¶”ë¡ ")
    
    test_path = os.path.join(args.dataset_dir, "test.csv")
    if not os.path.exists(test_path):
        print(f"Error: {test_path} not found!")
        return
    
    print("1. ë‹¨ì¼ ëª¨ë¸ë“¤ë¡œ test.csv ì¶”ë¡  ì¤‘...")
    
    for i, model_name in enumerate(all_models, 1):
        model_short = model_name.split('/')[-1]
        print(f"\n[{i}/{len(all_models)}] {model_short}...", end=' ')
        
        try:
            acc, f1, record_time = single_model_inference(args, model_name, "test.csv", save_jsonl=True)
            print(f"âœ“ JSONL ìƒì„±ë¨")
        except Exception as e:
            print(f"âœ— ì‹¤íŒ¨")
    
    print("\n2. ì•™ìƒë¸” ì¶”ë¡  ì¤‘...")
    record_time = datetime.now().strftime("%m_%d_%H_%M")
    
    try:
        ensemble_inference(args, all_models, record_time, "test.csv", save_jsonl=True)
        print("âœ“ ì•™ìƒë¸” JSONL ìƒì„±ë¨")
    except Exception as e:
        print(f"âœ— ì•™ìƒë¸” ì‹¤íŒ¨")
    
    print_header("ì œì¶œìš© íŒŒì¼ ìƒì„± ì™„ë£Œ")
    print("./prediction/ í´ë” í™•ì¸:")
    print("- ë‹¨ì¼ ëª¨ë¸: [ëª¨ë¸ëª…]_[ì‹œê°„].jsonl")
    print("- ì•™ìƒë¸”: hard_voting_ensemble_[ê°œìˆ˜]_[ì‹œê°„].jsonl")
    print("           soft_voting_ensemble_[ê°œìˆ˜]_[ì‹œê°„].jsonl")


if __name__ == "__main__":
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    args = parse_args()
    
    all_models = [
        "team-lucid/deberta-v3-base-korean",
        "monologg/kobert",
        "beomi/KcELECTRA-base-v2022",
        "snunlp/KR-ELECTRA-discriminator",
        "kykim/electra-kor-base"
    ]
    
    if args.mode == "train":
        os.environ.pop("WANDB_MODE", None)
        os.environ.pop("WANDB_DISABLED", None)
        
        print("="*80)
        print(f"ëª¨ë¸ í•™ìŠµ: {args.model_name}")
        print("="*80)
        print(f"â€¢ Run Name:      {args.run_name}")
        print(f"â€¢ Epochs:        {args.epochs}")
        print(f"â€¢ Batch Size:    {args.batch_size}")
        print(f"â€¢ Learning Rate: {args.lr}")
        print(f"â€¢ Dataset:       {args.dataset_dir}")
        print("="*80)
        
        import wandb
        print("\nğŸ“Š WandB ì´ˆê¸°í™” ì¤‘...")
        wandb.init(
            project="ssac_hate_speech",
            name=args.run_name,
            config={
                "model_name": args.model_name,
                "epochs": args.epochs,
                "batch_size": args.batch_size,
                "learning_rate": args.lr,
                "max_len": args.max_len,
            }
        )
        print(f"ğŸ“ˆ WandB ëŒ€ì‹œë³´ë“œ: {wandb.run.get_url()}")
        print("="*80)
        
        train(args, silent=False)
        wandb.finish()
        
        print("\n" + "="*80)
        print("âœ“ í•™ìŠµ ì™„ë£Œ!")
        print("="*80)
        
    else:
        os.environ["WANDB_MODE"] = "disabled"
        os.environ["WANDB_DISABLED"] = "true"
        
        if args.mode == "single_inference":
            model_performances = evaluate_single_models(args, all_models)
                    
        elif args.mode == "advanced_ensemble":
            print_header("í•œêµ­ì–´ í˜ì˜¤ í‘œí˜„ ë¶„ë¥˜ ëª¨ë¸ ì•™ìƒë¸” ì‹œìŠ¤í…œ")
            print("ëª¨ë¸ ëª©ë¡:")
            for i, model in enumerate(all_models, 1):
                print(f"   {i}. {model}")
            run_advanced_ensemble(args, all_models)
        
        elif args.mode == "test_submission":
            run_test_submission(args, all_models)
        
        else:
            print(f"ì˜¤ë¥˜: ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ '{args.mode}'")
